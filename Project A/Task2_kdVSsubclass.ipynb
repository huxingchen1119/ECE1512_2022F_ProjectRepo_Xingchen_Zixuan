{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90f00be",
   "metadata": {
    "id": "b90f00be"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense, Input, Lambda, Conv2D, MaxPooling2D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "_pdQ5-Z7KoOP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pdQ5-Z7KoOP",
    "outputId": "d716372e-b43b-4e69-c00f-0ba6307d4625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/1512/PA/mhist_dataset.zip\", 'r')\n",
    "zip_ref.extractall(\"./\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40af0213",
   "metadata": {
    "id": "40af0213"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224, 3)\n",
    "NUM_CLASSES = 2\n",
    "learning_rate_teacher_init = 1e-4\n",
    "learning_rate_student_init = 1e-3\n",
    "NUM_EPOCHS_init = 10\n",
    "NUM_EPOCHS_ft = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21672b14",
   "metadata": {
    "id": "21672b14"
   },
   "source": [
    "# Load pre-trained ResNet50V2 and MobileNetV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da57cb10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da57cb10",
    "outputId": "7801fe18-06b3-4a2f-b13e-03fa0852017f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102869336/102869336 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "14536120/14536120 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet_model = ResNet50V2(include_top=True, weights='imagenet', input_shape=IMAGE_SIZE)\n",
    "#freeze pre-trained resnet layers for initial epochs\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Design the last dense layer\n",
    "x_teacher = Dense(2,activation=None)(resnet_model.output)\n",
    "x_teacher.trainable = True\n",
    "\n",
    "teacher_model_kd = Model(inputs=resnet_model.input, outputs=x_teacher)\n",
    "\n",
    "mobile_model = MobileNetV2(include_top=True, weights='imagenet', input_shape=IMAGE_SIZE)\n",
    "#freeze pre-trained mobilenet layers for initial epochs\n",
    "for layer in mobile_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Design the last dense layer\n",
    "x_student = Dense(2,activation=None)(mobile_model.output)\n",
    "x_student.trainable = True\n",
    "\n",
    "student_model_kd = Model(inputs=mobile_model.input, outputs=x_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6af22",
   "metadata": {
    "id": "dec6af22"
   },
   "source": [
    "# Custom dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0b8061",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de0b8061",
    "outputId": "2b3c613b-bfca-4e60-af91-ba8d912de000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2175\n",
      "977\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "csv_path = './annotations.csv'\n",
    "image_path = './images/images'\n",
    "# model(torch.tensor(np.ones((1, 224,224,3))), training=True)\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, image_path, csv_path, transformer, split):\n",
    "        self.image_path = image_path\n",
    "        self.csv_path = csv_path\n",
    "        annotations = pd.read_csv(csv_path)\n",
    "        image_names = annotations ['Image Name']\n",
    "        majority_vote_labels = annotations ['Majority Vote Label']\n",
    "        self.train_image_names = np.array(image_names[annotations['Partition']=='train'])\n",
    "        self.train_mv_labels = np.array(majority_vote_labels[annotations['Partition']=='train'])      \n",
    "        self.test_image_names = np.array(image_names[annotations['Partition']=='test'])\n",
    "        self.test_mv_labels = np.array(majority_vote_labels[annotations['Partition']=='test'])       \n",
    "        self.transformer = transformer\n",
    "        self.split = split\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.split == 'train':\n",
    "            return len(self.train_mv_labels)\n",
    "        else:\n",
    "            return len(self.test_mv_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'train':        \n",
    "            image_full_name = os.path.join(self.image_path, self.train_image_names[idx])\n",
    "            x = Image.open(image_full_name)\n",
    "            if self.transformer is not None:\n",
    "                x = self.transformer(x)\n",
    "                x = x.permute(1,2,0)\n",
    "            \n",
    "            label = self.train_mv_labels[idx]\n",
    "            if label == 'HP':\n",
    "                y = np.array([1, 0])\n",
    "            else:\n",
    "                y = np.array([0, 1])\n",
    "                \n",
    "        else:        \n",
    "            image_full_name = os.path.join(self.image_path, self.test_image_names[idx])\n",
    "            x = Image.open(image_full_name)\n",
    "            if self.transformer is not None:\n",
    "                x = self.transformer(x)\n",
    "                x = x.permute(1,2,0)\n",
    "\n",
    "            label = self.test_mv_labels[idx]\n",
    "            if label == 'HP':\n",
    "                y = np.array([1, 0])\n",
    "            else:\n",
    "                y = np.array([0, 1])\n",
    "                \n",
    "        return x, y\n",
    "    \n",
    "custom_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomRotation(degrees=15),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "custom_transformer_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "      \n",
    "train_dataset = Custom_Dataset(image_path, csv_path, custom_transformer, 'train')\n",
    "test_dataset = Custom_Dataset(image_path, csv_path, custom_transformer_test, 'test')\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83703c57",
   "metadata": {
    "id": "83703c57"
   },
   "source": [
    "# Teacher loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23cc417",
   "metadata": {
    "id": "e23cc417"
   },
   "outputs": [],
   "source": [
    "def compute_teacher_loss(images, labels):\n",
    "\n",
    "    subclass_logits = teacher_model_kd(images, training=True)\n",
    "\n",
    "    cross_entropy_loss_value = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          labels, subclass_logits))\n",
    "\n",
    "    return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29065c",
   "metadata": {
    "id": "ce29065c"
   },
   "source": [
    "# Student loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3159061",
   "metadata": {
    "id": "e3159061"
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE = 4 #temperature hyperparameter\n",
    "\n",
    "def distillation_loss(teacher_logits, student_logits, temperature = DISTILLATION_TEMPERATURE):\n",
    "\n",
    "    soft_targets = tf.nn.softmax(teacher_logits/temperature, axis = 1)\n",
    "    \n",
    "    return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "def compute_student_loss(images, labels):\n",
    "\n",
    "    student_subclass_logits = student_model_kd(images, training=True)\n",
    "    teacher_subclass_logits = teacher_model_kd(images, training=False)\n",
    "    distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "    cross_entropy_loss_value = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          labels, student_subclass_logits))\n",
    "    \n",
    "    total_loss = cross_entropy_loss_value * ALPHA + (1 - ALPHA) * distillation_loss_value\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad8c5d",
   "metadata": {
    "id": "2fad8c5d"
   },
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e1ab30",
   "metadata": {
    "id": "82e1ab30"
   },
   "outputs": [],
   "source": [
    "def tf_reduceat(data, at_array, axis=-1):\n",
    "    split_data = tf.split(data, at_array, axis=axis)\n",
    "    return tf.stack([tf.reduce_sum(i, axis=axis) for i in split_data], axis=axis)\n",
    "    \n",
    "# def compute_num_correct(model, images, labels):\n",
    "#     class_logits = model(images, training=False)\n",
    "#     if class_logits.shape[1] != NUM_CLASSES:\n",
    "#         class_logits = tf_reduceat(class_logits, [8, 8])\n",
    "        \n",
    "#     return tf.reduce_sum(\n",
    "#       tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "#               tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "def compute_auc(model, images, labels):\n",
    "    class_logits = model(images, training=False)\n",
    "    if class_logits.shape[1] != NUM_CLASSES:\n",
    "        class_logits = tf_reduceat(class_logits, [8, 8])\n",
    "    \n",
    "    scores = tf.nn.sigmoid(class_logits)\n",
    "    m1 = tf.keras.metrics.AUC()\n",
    "    m1.reset_state()\n",
    "    m1.update_state(labels, scores)\n",
    "    return m1.result().numpy()\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn, learning_rate, train_loader, test_loader, num_total):\n",
    "    print('Start initial epochs')\n",
    "    # do initial training epochs\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    for epoch in range(1, NUM_EPOCHS_init + 1):\n",
    "        # Run training.\n",
    "        print('Epoch {}: '.format(epoch), end='')\n",
    "        for images, labels in train_loader:\n",
    "            with tf.GradientTape() as tape:\n",
    "                images = tf.convert_to_tensor(images.numpy())\n",
    "                labels = tf.convert_to_tensor(labels.numpy())\n",
    "                loss_value = compute_loss_fn(images, labels)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    #unfreezing all layers\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    print('\\n')\n",
    "    print('Start fine-tuning epochs')\n",
    "    # do initial training epochs\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate/10)\n",
    "    for epoch in range(1, NUM_EPOCHS_ft + 1):\n",
    "        # Run training.\n",
    "        print('Epoch {}: '.format(epoch), end='')\n",
    "        for images, labels in train_loader:\n",
    "            with tf.GradientTape() as tape:\n",
    "                images = tf.convert_to_tensor(images.numpy())\n",
    "                labels = tf.convert_to_tensor(labels.numpy())\n",
    "                loss_value = compute_loss_fn(images, labels)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    AUC_set = []\n",
    "    for images, labels in test_loader:\n",
    "        images = tf.convert_to_tensor(images.numpy())\n",
    "        labels = tf.convert_to_tensor(labels.numpy())\n",
    "        AUC_set.append(compute_auc(model, images, labels))\n",
    "\n",
    "    test_AUC = sum(AUC_set)/len(AUC_set)\n",
    "    print('\\n')\n",
    "    print(\"Test AUC: \" + '{:.2f}'.format(test_AUC))\n",
    "    return test_AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RG5EF9veZSwa",
   "metadata": {
    "id": "RG5EF9veZSwa"
   },
   "source": [
    "# Knowledge Distallation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e1857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d54e1857",
    "outputId": "98b852ad-ae9b-4442-e475-7b761df2975f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training teacher model\n",
      "Start initial epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: \n",
      "\n",
      "Start fine-tuning epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: Epoch 11: Epoch 12: Epoch 13: Epoch 14: Epoch 15: Epoch 16: Epoch 17: Epoch 18: Epoch 19: Epoch 20: Epoch 21: Epoch 22: Epoch 23: Epoch 24: Epoch 25: \n",
      "\n",
      "Test AUC: 0.76\n",
      "Test AUC for teacher model is 0.756162241101265\n",
      "-------------------------------------\n",
      "Start training student model\n",
      "Start initial epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: \n",
      "\n",
      "Start fine-tuning epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: Epoch 11: Epoch 12: Epoch 13: Epoch 14: Epoch 15: Epoch 16: Epoch 17: Epoch 18: Epoch 19: Epoch 20: Epoch 21: Epoch 22: Epoch 23: Epoch 24: Epoch 25: \n",
      "\n",
      "Test AUC: 0.70\n",
      "Test AUC for student model is 0.7046599330440644\n"
     ]
    }
   ],
   "source": [
    "print('Start training teacher model')\n",
    "test_acc_teacher = train_and_evaluate(teacher_model_kd, compute_teacher_loss, learning_rate_teacher_init, train_loader, test_loader, len(test_dataset))\n",
    "print('Test AUC for teacher model is {}'.format(test_acc_teacher))\n",
    "print('-------------------------------------')\n",
    "print('Start training student model')\n",
    "test_acc_student = train_and_evaluate(student_model_kd, compute_student_loss, learning_rate_student_init, train_loader, test_loader, len(test_dataset))\n",
    "print('Test AUC for student model is {}'.format(test_acc_student))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g6__E33UcmuX",
   "metadata": {
    "id": "g6__E33UcmuX"
   },
   "source": [
    "# Subclass Distallation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5918c023",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5918c023",
    "outputId": "60d0d60d-a84e-457c-d349-b7afb8f5485f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training teacher model\n",
      "Start initial epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: \n",
      "\n",
      "Start fine-tuning epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: Epoch 11: Epoch 12: Epoch 13: Epoch 14: Epoch 15: Epoch 16: Epoch 17: Epoch 18: Epoch 19: Epoch 20: Epoch 21: Epoch 22: Epoch 23: Epoch 24: Epoch 25: \n",
      "\n",
      "Test AUC: 0.89\n",
      "Test AUC for teacher model is 0.8907946655827184\n",
      "-------------------------------------\n",
      "Start training student model\n",
      "Start initial epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: \n",
      "\n",
      "Start fine-tuning epochs\n",
      "Epoch 1: Epoch 2: Epoch 3: Epoch 4: Epoch 5: Epoch 6: Epoch 7: Epoch 8: Epoch 9: Epoch 10: Epoch 11: Epoch 12: Epoch 13: Epoch 14: Epoch 15: Epoch 16: Epoch 17: Epoch 18: Epoch 19: Epoch 20: Epoch 21: Epoch 22: Epoch 23: Epoch 24: Epoch 25: \n",
      "\n",
      "Test AUC: 0.90\n",
      "Test AUC for student model is 0.9044984752132047\n"
     ]
    }
   ],
   "source": [
    "DISTILLATION_TEMPERATURE = 16\n",
    "\n",
    "resnet_model = ResNet50V2(include_top=True, weights='imagenet', input_shape=IMAGE_SIZE)\n",
    "#freeze pre-trained resnet layers for initial epochs\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Design the last dense layer\n",
    "x_teacher = Dense(16, activation=None)(resnet_model.output)\n",
    "x_teacher.trainable = True\n",
    "\n",
    "teacher_model_SbD = Model(inputs=resnet_model.input, outputs=x_teacher)\n",
    "\n",
    "mobile_model = MobileNetV2(include_top=True, weights='imagenet', input_shape=IMAGE_SIZE)\n",
    "#freeze pre-trained mobilenet layers for initial epochs\n",
    "for layer in mobile_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Design the last dense layer\n",
    "x_student = Dense(16, activation=None)(mobile_model.output)\n",
    "x_student.trainable = True\n",
    "\n",
    "student_model_SbD = Model(inputs=mobile_model.input, outputs=x_student)\n",
    "\n",
    "def compute_teacher_loss_SbD(images, labels):     \n",
    "        \n",
    "    subclass_logits_Z = teacher_model_SbD(images, training=True)\n",
    "    subclass_logits_P = tf.nn.softmax(subclass_logits_Z)\n",
    "    subclass_logits_P_sumed = tf_reduceat(subclass_logits_P, [8, 8])\n",
    "    \n",
    "    cee = tf.keras.losses.CategoricalCrossentropy()\n",
    "    cross_entropy_loss_value = cee(labels, subclass_logits_P_sumed)\n",
    "    temperature = 120\n",
    "    BETA = 0.1\n",
    "    \n",
    "    def Auxiliary_loss(subclass_logits_Z, temperature):\n",
    "        mean = tf.math.reduce_mean(subclass_logits_Z, axis = 1)\n",
    "        var = tf.math.reduce_variance(subclass_logits_Z, axis = 1)\n",
    "        normalizer = tf.keras.layers.Normalization(axis=0, mean=mean, variance=var)\n",
    "        subclass_logits_Z = normalizer(subclass_logits_Z)\n",
    "\n",
    "        subclass_logits_Z_transposed = tf.transpose(subclass_logits_Z)\n",
    "        logics_vectors = tf.matmul(subclass_logits_Z, subclass_logits_Z_transposed)\n",
    "        logics_vectors /= temperature\n",
    "        exp_values = tf.math.exp(logics_vectors)\n",
    "        log_term = tf.reduce_sum(exp_values, 1)\n",
    "        log_results = tf.math.log(log_term)\n",
    "        log_results -= (1/temperature)\n",
    "        log_results -= math.log(batch_size)\n",
    "        auxiliary_loss = tf.reduce_mean(log_results)\n",
    "        return auxiliary_loss\n",
    "    \n",
    "    auxiliary_loss = Auxiliary_loss(subclass_logits_Z, temperature)\n",
    "    teacher_loss = cross_entropy_loss_value + BETA * auxiliary_loss\n",
    "    return teacher_loss\n",
    "\n",
    "def distillation_loss_SbD(teacher_logits, student_logits, temperature = DISTILLATION_TEMPERATURE):\n",
    "\n",
    "    soft_targets = tf.nn.softmax(teacher_logits/temperature, axis = 1)\n",
    "    \n",
    "    return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "def compute_student_loss_SbD(images, labels):\n",
    "\n",
    "    student_subclass_logits_Z = student_model_SbD(images, training=True)\n",
    "\n",
    "    teacher_subclass_logits_Z = teacher_model_SbD(images, training=False)\n",
    "    \n",
    "    student_subclass_logits_P = tf.nn.softmax(student_subclass_logits_Z)\n",
    "    student_subclass_logits_P_sumed = tf_reduceat(student_subclass_logits_P, [8, 8])\n",
    "    \n",
    "    cee = tf.keras.losses.CategoricalCrossentropy()\n",
    "    cross_entropy_loss_value = cee(labels, student_subclass_logits_P_sumed)\n",
    "    \n",
    "    distillation_loss_value = distillation_loss_SbD(teacher_subclass_logits_Z, student_subclass_logits_Z, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "    student_loss = cross_entropy_loss_value * ALPHA + (1 - ALPHA) * distillation_loss_value\n",
    "\n",
    "    return student_loss\n",
    "\n",
    "print('Start training teacher model')\n",
    "test_acc_teacher = train_and_evaluate(teacher_model_SbD, compute_teacher_loss_SbD, learning_rate_teacher_init, train_loader, test_loader, len(test_dataset))\n",
    "print('Test AUC for teacher model is {}'.format(test_acc_teacher))\n",
    "print('-------------------------------------')\n",
    "print('Start training student model')\n",
    "test_acc_student = train_and_evaluate(student_model_SbD, compute_student_loss_SbD, learning_rate_student_init, train_loader, test_loader, len(test_dataset))\n",
    "print('Test AUC for student model is {}'.format(test_acc_student))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lFaxjODGg_gh",
   "metadata": {
    "id": "lFaxjODGg_gh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
